---
title: "Azure Application Insights"
---

Traceloop supports sending traces to Azure Application Insights via standard OpenTelemetry integrations.

Review how to setup [OpenTelemetry with Python in Azure Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-enable?tabs=python).

<Frame>
  <img src="/img/integrations/azure.png" />
</Frame>

1. Provision an Application Insights instance in the [Azure portal](https://portal.azure.com/).
2. Get your Connection String from the instance - [details here](https://learn.microsoft.com/en-us/azure/azure-monitor/app/sdk-connection-string?tabs=python).
3. Install required packages

```bash
pip install azure-monitor-opentelemetry-exporter traceloop-sdk
```

4. Example implementation

```python
import os
from traceloop.sdk import Traceloop
from traceloop.sdk.decorators import workflow
from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter

from openai import OpenAI  # Ensure you have the openai library installed

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

# Configure the tracer provider to export traces to Azure Application Insights.
# Get your complete connection string from the Azure Portal or CLI.
exporter = AzureMonitorTraceExporter(connection_string="INSERT_CONNECTION_STRING_HERE")

# Pass your exporter to Traceloop
Traceloop.init(app_name="your_app_name", exporter=exporter)


@workflow(name="llm_execution")
def execute_llm():
    # Replace with your actual OpenAI API call
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Hello world"}],
        max_tokens=60
    )

    return response.choices[0].message.content

if __name__ == "__main__":
    execute_llm()
```
